{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66806df",
   "metadata": {},
   "source": [
    "\n",
    "# <center> Escuela Politécnica Nacional </center>\n",
    "\n",
    "**Nombre:** Francisco Ulloa<br>\n",
    "**Fecha:** Quito, 28 de enero de 2026<br>\n",
    "**Tema:** Factoreo en transformers  <br>\n",
    "**Repositorio:**<br>\n",
    "https://github.com/Fu5CHAR/Metodos_numericos_2025B_Ulloa-Francisco/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa41adb",
   "metadata": {},
   "source": [
    "# Uso del factoreo de matrices en arquitecturas Transformer\n",
    "\n",
    "## ¿Qué es el factoreo de matrices en Transformers?\n",
    "\n",
    "En redes neuronales **Transformer**, el factoreo de matrices consiste en **descomponer una matriz grande en el producto de matrices más pequeñas**, normalmente de bajo rango:\n",
    "\n",
    "$$\n",
    "W \\in \\mathbb{R}^{d \\times d} \\Rightarrow W \\approx A \\cdot B\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $$A \\in \\mathbb{R}^{d \\times k}$$\n",
    "- $$B \\in \\mathbb{R}^{k \\times d}$$\n",
    "- $$con k \\ll d$$\n",
    "\n",
    "\n",
    "Este enfoque aparece explícita o implícitamente en:\n",
    "- Low-Rank Factorization\n",
    "- Attention factorization\n",
    "- Proyecciones Q, K y V\n",
    "- Adapter layers\n",
    "- LoRA (Low-Rank Adaptation)\n",
    "- Transformers eficientes (Linformer, Performer, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Dónde se usa el factoreo de matrices en Transformers?\n",
    "\n",
    "### 1. Factoreo en el mecanismo de atención\n",
    "\n",
    "La atención estándar se define como:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) =\n",
    "\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Este cálculo tiene complejidad **cuadrática** $$O(n^2)$$ respecto a la longitud de la secuencia.\n",
    "\n",
    "Mediante factoreo o proyecciones de bajo rango:\n",
    "- Se reduce el costo computacional\n",
    "- Se reduce el consumo de memoria\n",
    "- Se habilita el uso de secuencias más largas\n",
    "\n",
    "Ejemplo: **Linformer** aproxima la matriz de atención usando proyecciones de bajo rango.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Factoreo en capas lineales (Feed-Forward)\n",
    "\n",
    "Las capas Feed-Forward contienen matrices grandes:\n",
    "\n",
    "$$\n",
    "W \\in \\mathbb{R}^{d_{model} \\times d_{ff}}\n",
    "$$\n",
    "\n",
    "Estas matrices pueden factorizarse como:\n",
    "\n",
    "$$\n",
    "W = A \\cdot B\n",
    "$$\n",
    "\n",
    "Esto se usa para:\n",
    "- Compresión del modelo\n",
    "- Reducción de parámetros\n",
    "- Aceleración de inferencia\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Factoreo en fine-tuning eficiente (LoRA)\n",
    "\n",
    "En **LoRA**, los pesos originales del modelo se congelan y se añaden matrices de bajo rango entrenables:\n",
    "\n",
    "$$\n",
    "W' = W + A \\cdot B\n",
    "$$\n",
    "\n",
    "Ventajas:\n",
    "- Se entrenan pocos parámetros\n",
    "- No se altera el modelo base\n",
    "- Permite adaptar modelos grandes de forma eficiente\n",
    "\n",
    "---\n",
    "\n",
    "## Razones para usar factoreo de matrices en Transformers\n",
    "\n",
    "### 1. Reducción del costo computacional\n",
    "- Las multiplicaciones matriciales grandes son costosas\n",
    "- El factoreo reduce la complejidad de:\n",
    "$$\n",
    "O(d^2) \\rightarrow O(d \\cdot k)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Ahorro de memoria\n",
    "- Menor número de parámetros\n",
    "- Menor uso de memoria de activaciones\n",
    "- Clave para entrenamiento en GPUs\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Escalabilidad del modelo\n",
    "- Permite:\n",
    "  - Modelos más grandes\n",
    "  - Secuencias más largas\n",
    "  - Mayor profundidad\n",
    "- Sin crecimiento explosivo del costo computacional\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Regularización implícita\n",
    "- El bajo rango actúa como restricción\n",
    "- Reduce el sobreajuste\n",
    "- Mejora la generalización\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Eficiencia en fine-tuning\n",
    "- Se entrenan solo matrices pequeñas\n",
    "- Reduce tiempo y costo de entrenamiento\n",
    "- Ideal para personalización de LLMs\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Mejor aprovechamiento del hardware\n",
    "- GPUs modernas están optimizadas para multiplicaciones matriciales pequeñas\n",
    "- Mejor uso de Tensor Cores (FP16, FP8)\n",
    "\n",
    "---\n",
    "\n",
    "## Ventajas prácticas del factoreo de matrices\n",
    "\n",
    "| Ventaja | Impacto |\n",
    "|-------|--------|\n",
    "| Menos parámetros | Modelos más ligeros |\n",
    "| Menor latencia | Inferencia más rápida |\n",
    "| Menor consumo energético | Mayor eficiencia |\n",
    "| Mayor escalabilidad | Secuencias largas |\n",
    "| Fine-tuning económico | Menor costo |\n",
    "| Regularización | Mejor generalización |\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusión\n",
    "\n",
    "El factoreo de matrices es una **estrategia clave** en los Transformers modernos que permite:\n",
    "\n",
    "- Reducir costos computacionales\n",
    "- Escalar modelos de lenguaje grandes\n",
    "- Adaptar modelos sin reentrenamiento completo\n",
    "- Aprovechar hardware especializado\n",
    "\n",
    "Sin factoreo de matrices, los Transformers modernos no serían viables a gran escala.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
