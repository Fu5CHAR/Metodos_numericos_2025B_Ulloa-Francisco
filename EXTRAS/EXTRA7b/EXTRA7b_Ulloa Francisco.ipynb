{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377fb5b0",
   "metadata": {},
   "source": [
    "\n",
    "# <center> Escuela Politécnica Nacional </center>\n",
    "\n",
    "**Nombre:** Francisco Ulloa<br>\n",
    "**Fecha:** Quito, 28 de enero de 2026<br>\n",
    "**Tema:** GPUs Hopper vs Blackwell  <br>\n",
    "**Repositorio:**<br>\n",
    "https://github.com/Fu5CHAR/Metodos_numericos_2025B_Ulloa-Francisco/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a9e7a",
   "metadata": {},
   "source": [
    "# Comparación de arquitecturas NVIDIA: Hopper vs Blackwell\n",
    "\n",
    "## 1. Diferencias generales entre Hopper y Blackwell\n",
    "\n",
    "### Arquitectura Hopper\n",
    "- Lanzada en 2022–2023.\n",
    "- Orientada a **IA generativa y computación de alto rendimiento (HPC)**.\n",
    "- Introduce el **Transformer Engine**, que usa múltiples precisiones (FP16, BF16, FP8).\n",
    "- Incorpora **Tensor Cores de cuarta generación**.\n",
    "- Soporta aceleración avanzada para entrenamiento de modelos grandes (LLM).\n",
    "\n",
    "### Arquitectura Blackwell\n",
    "- Lanzada en 2024–2025.\n",
    "- Sucesora directa de Hopper.\n",
    "- Incorpora **Tensor Cores de quinta generación**.\n",
    "- Introduce soporte nativo para **precisiones ultra-bajas (FP6 y FP4)**.\n",
    "- Mayor eficiencia energética, más ancho de banda y mejor escalabilidad.\n",
    "- Pensada para **IA generativa masiva e inferencia a gran escala**.\n",
    "\n",
    "### Comparación resumida\n",
    "\n",
    "| Característica | Hopper | Blackwell |\n",
    "|---------------|--------|-----------|\n",
    "| Generación Tensor Core | 4ª | 5ª |\n",
    "| Enfoque principal | Entrenamiento IA + HPC | IA generativa eficiente |\n",
    "| Precisión mínima | FP8 | FP4 |\n",
    "| Eficiencia energética | Alta | Muy alta |\n",
    "| Escalabilidad | Alta | Superior |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Diferencia entre FP32 y TF32 (a veces llamado TP32)\n",
    "\n",
    "### FP32 (Floating Point 32)\n",
    "- Formato estándar IEEE 754.\n",
    "- Usa **32 bits**:\n",
    "  - 1 bit de signo\n",
    "  - 8 bits de exponente\n",
    "  - 23 bits de mantisa\n",
    "- Alta precisión numérica.\n",
    "- Usado tradicionalmente en computación científica.\n",
    "\n",
    "### TF32 (TensorFloat-32)\n",
    "- Formato introducido por NVIDIA para **Tensor Cores**.\n",
    "- Mantiene:\n",
    "  - 8 bits de exponente (mismo rango que FP32)\n",
    "- Reduce:\n",
    "  - Mantisa a ~10 bits\n",
    "- No es un formato de almacenamiento, sino **un modo de cálculo acelerado**.\n",
    "- Aumenta significativamente el rendimiento con mínima pérdida de precisión.\n",
    "\n",
    "### Comparación FP32 vs TF32\n",
    "\n",
    "| Formato | Bits totales | Exponente | Mantisa | Precisión | Rendimiento |\n",
    "|-------|--------------|-----------|---------|-----------|-------------|\n",
    "| FP32 | 32 | 8 | 23 | Alta | Medio |\n",
    "| TF32 | 32 | 8 | ~10 | Media | Muy alto |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Representaciones de datos soportadas\n",
    "\n",
    "### Hopper\n",
    "\n",
    "- **CUDA Cores**\n",
    "  - FP64\n",
    "  - FP32\n",
    "  - FP16\n",
    "  - BF16\n",
    "  - INT8\n",
    "\n",
    "- **Tensor Cores**\n",
    "  - FP64\n",
    "  - TF32\n",
    "  - FP16\n",
    "  - BF16\n",
    "  - FP8\n",
    "  - INT8\n",
    "\n",
    "### Blackwell\n",
    "\n",
    "- **CUDA Cores**\n",
    "  - FP64\n",
    "  - FP32\n",
    "  - FP16\n",
    "  - BF16\n",
    "\n",
    "- **Tensor Cores**\n",
    "  - FP64\n",
    "  - TF32\n",
    "  - FP16\n",
    "  - BF16\n",
    "  - FP8\n",
    "  - FP6\n",
    "  - FP4\n",
    "  - INT8\n",
    "\n",
    "### Comparación de soporte\n",
    "\n",
    "| Precisión | Hopper | Blackwell |\n",
    "|---------|--------|-----------|\n",
    "| FP64 | ✔ | ✔ |\n",
    "| FP32 | ✔ | ✔ |\n",
    "| TF32 | ✔ | ✔ |\n",
    "| FP16 | ✔ | ✔ |\n",
    "| BF16 | ✔ | ✔ |\n",
    "| FP8 | ✔ | ✔ |\n",
    "| FP6 | ✖ | ✔ |\n",
    "| FP4 | ✖ | ✔ |\n",
    "| INT8 | ✔ | ✔ |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ¿Por qué la nueva arquitectura prefiere menor precisión?\n",
    "\n",
    "### 1. Mayor rendimiento computacional\n",
    "- Menos bits permiten **más operaciones por ciclo de reloj**.\n",
    "- Se incrementan los FLOPS efectivos.\n",
    "\n",
    "### 2. Menor uso de memoria y ancho de banda\n",
    "- Datos más pequeños reducen:\n",
    "  - Acceso a memoria\n",
    "  - Consumo de ancho de banda\n",
    "- Factor crítico en modelos grandes.\n",
    "\n",
    "### 3. Tolerancia de los modelos de IA\n",
    "- Redes neuronales profundas **no requieren precisión completa** en la mayoría de capas.\n",
    "- La reducción de precisión no degrada significativamente la calidad del modelo.\n",
    "\n",
    "### 4. Mejor eficiencia energética\n",
    "- Menor consumo por operación.\n",
    "- Reducción de costos operativos en centros de datos.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusión\n",
    "\n",
    "- **Hopper** introduce la aceleración avanzada con FP8 y TF32 para IA moderna.\n",
    "- **Blackwell** extiende este enfoque hacia **precisiones ultra-bajas (FP4, FP6)**.\n",
    "- El uso de menor precisión permite:\n",
    "  - Más rendimiento\n",
    "  - Menor consumo energético\n",
    "  - Escalabilidad para IA generativa masiva\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
